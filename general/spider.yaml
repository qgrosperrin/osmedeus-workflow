name: spider
desc: Crawling links in http site

report:
  final:
    - "{{Output}}/linkfinding/links-{{Workspace}}.txt"
    - "{{Output}}/linkfinding/trufflehog-{{Workspace}}-output.txt"

params:
  - httpFile: "{{Output}}/probing/http-{{Workspace}}.txt"
  - linkFile: "{{Output}}/linkfinding/links-{{Workspace}}.txt"
  - httpResponse: "{{Output}}/http-response/"
  - spiderTimeout: "2h"
  - spiderThreads: "{{ threads }}"
  - spiderPrallel: "{{ threads / 2 }}"
  - spiderDepth: "3"
  - crawlingTime: "30"
  - skipSpidering: "false"
  - defaultUA: "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
  - trufflehogThreads: "{{ threads * 6}}"
  - httpThreads: "{{ threads * 10}}"

pre_run:
  - CreateFolder("{{Output}}/linkfinding")

steps:
  - conditions:
      - '"{{skipSpidering}}" == "true"'
    scripts:
      - ErrPrintf("Filter", "Skipping spidering")
      - Exit(1)

  - required:
      - "{{Binaries}}/katana"
      - "{{httpFile}}"
    source: "{{httpFile}}"
    threads: '{{spiderPrallel}}'
    commands:
      - "timeout -k 1m {{spiderTimeout}} {{Binaries}}/katana -c {{spiderThreads}} -jc -ct {{crawlingTime}} -u [[.line]] | sort -u  >> {{linkFile}}"

  - label: "Getting raw response & scanning for secrets with trufflehog"
    scripts:
      - ExecCmd("cat {{linkFile}} | {{Binaries}}/httpx -silent -nf -no-color -title -t {{httpThreads}} -H '{{defaultUA}}' --store-response-dir {{httpResponse}} ")
      - ExecCmd("trufflehog --concurrency={{trufflehogThreads}} filesystem {{httpResponse}} > {{Output}}/linkfinding/trufflehog-{{Workspace}}-output.txt 2>&1")
  
  - label: "Getting JSON response for better display"
    scripts:
      - ExecCmd("cat {{linkFile}} | {{Binaries}}/httpx ffuf -silent -t {{httpThreads}} -noninteractive -ac -acs advanced -timeout 15 -se -D -fc '429,404,400' -H '{{defaultUA}}' -json > {{Output}}/linkfinding/raw-[[._id_]].json 2>/dev/null")
      - ExecCmd("cat {{Output}}/linkfinding/raw-*.json | jq -r '[.url,(.status|tostring),(.length|tostring),(.words|tostring),(.lines|tostring),.redirectlocation] | join(\",\")' > {{Output}}/linkfinding/beautify-{{Workspace}}.csv")
      - ExecCmd("cat {{Output}}/linkfinding/raw-*.json | {{Binaries}}/json-cleaner -f status,words,lines | jq -r '[.url,(.status|tostring),(.length|tostring),(.words|tostring),(.lines|tostring),.redirectlocation] | join(\",\")' > {{Output}}/linkfinding/unique-beautify-{{Workspace}}.csv")
      - ExecCmd("rm -rf {{Output}}/linkfinding/raw-*.json")
      # beautify the result
      - ExecCmd("cat {{Output}}/linkfinding/beautify-{{Workspace}}.csv | {{Binaries}}/csvtk cut -f 1,2,3,4,5,6 -I | {{Binaries}}/csvtk pretty --no-header-row -I -s ' | ' -W 75 > {{Output}}/linkfinding/beautify-{{Workspace}}.txt")
      - ExecCmd("cat {{Output}}/linkfinding/unique-beautify-{{Workspace}}.csv | {{Binaries}}/csvtk cut -f 1,2,3,4,5,6 -I | {{Binaries}}/csvtk pretty --no-header-row -I -s ' | ' -W 75 > {{Output}}/linkfinding/unique-beautify-{{Workspace}}.txt")
      - Cat("{{Output}}/linkfinding/unique-beautify-{{Workspace}}.txt")
      - "Printf('==> The unfiltered result can be found at: {{Output}}/linkfinding/beautify-{{Workspace}}.txt')"


  # Print the file if there is not too much data
  - conditions:
      - "FileLength('{{linkFile}}') < 10000"
    scripts:
      - Cat("{{linkFile}}")